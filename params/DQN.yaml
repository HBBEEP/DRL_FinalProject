DQN : 
  experiment_1:
    save_path : output/DQN_policy_exp1
    reward_func : Full_score
    n_episodes : 20000
    hidden_dim : 2048
    target_update_interval : 20
    initial_epsilon : 0.9
    epsilon_decay : 0.985
    final_epsilon : 0.01
    learning_rate : 0.00005
    discount_factor :  0.99
    soft_update : false
    use_scheduler : true
    use_preset_board : false
    max_preset_tile : 4
    tau : 0.95
    batch_size : 64
    buffer_size : 50000

  experiment_2:
    save_path : output/DQN_policy_exp2
    reward_func : Merge_score
    n_episodes : 20000
    hidden_dim : 2048
    target_update_interval : 20
    initial_epsilon : 0.9
    epsilon_decay : 0.985
    final_epsilon : 0.01
    learning_rate : 0.00005
    discount_factor :  0.99
    soft_update : false
    use_scheduler : true
    use_preset_board : false
    max_preset_tile : 4
    tau : 0.95
    batch_size : 64
    buffer_size : 50000

  experiment_3:
    save_path : output/DQN_policy_exp3
    reward_func : Full_score
    n_episodes : 20000
    hidden_dim : 2048
    target_update_interval : 20
    initial_epsilon : 0.9
    epsilon_decay : 0.985
    final_epsilon : 0.05
    learning_rate : 0.00005
    discount_factor :  0.99
    soft_update : false
    use_scheduler : true
    use_preset_board : false
    max_preset_tile : 4
    tau : 0.95
    batch_size : 64
    buffer_size : 50000

  experiment_4:
    save_path : output/DQN_policy_exp4
    reward_func : Guide_score
    n_episodes : 20000
    hidden_dim : 2048
    target_update_interval : 20
    initial_epsilon : 0.9
    epsilon_decay : 0.985
    final_epsilon : 0.01
    learning_rate : 0.00005
    discount_factor :  0.99
    soft_update : false
    use_scheduler : true
    use_preset_board : false
    max_preset_tile : 4
    tau : 0.95
    batch_size : 64
    buffer_size : 50000

  experiment_5:
    save_path : output/DQN_policy_exp5_no_scheduler
    reward_func : Full_score
    n_episodes : 10000
    hidden_dim : 2048
    target_update_interval : 20
    initial_epsilon : 0.9
    epsilon_decay : 0.985
    final_epsilon : 0.01
    learning_rate : 0.00005
    discount_factor :  0.99
    soft_update : false
    use_scheduler : false
    use_preset_board : false
    max_preset_tile : 4
    tau : 0.95
    batch_size : 64
    buffer_size : 50000

  experiment_6:
    save_path : output/DQN_policy_exp6_no_scheduler
    reward_func : Merge_score
    n_episodes : 10000
    hidden_dim : 2048
    target_update_interval : 20
    initial_epsilon : 0.9
    epsilon_decay : 0.985
    final_epsilon : 0.01
    learning_rate : 0.00005
    discount_factor :  0.99
    soft_update : false
    use_scheduler : false
    use_preset_board : false
    max_preset_tile : 4
    tau : 0.95
    batch_size : 64
    buffer_size : 50000

  experiment_7:
    save_path : output/DQN_policy_exp7_preset_512
    reward_func : Full_score
    n_episodes : 10000
    hidden_dim : 2048
    target_update_interval : 20
    initial_epsilon : 0.9
    epsilon_decay : 0.985
    final_epsilon : 0.01
    learning_rate : 0.00005
    discount_factor :  0.99
    soft_update : false
    use_scheduler : true
    use_preset_board : true
    max_preset_tile : 512
    tau : 0.95
    batch_size : 64
    buffer_size : 50000

  experiment_8:
    save_path : output/DQN_policy_exp8_preset_1024
    reward_func : Full_score
    n_episodes : 10000
    hidden_dim : 2048
    target_update_interval : 20
    initial_epsilon : 0.9
    epsilon_decay : 0.985
    final_epsilon : 0.01
    learning_rate : 0.00005
    discount_factor :  0.99
    soft_update : false
    use_scheduler : true
    use_preset_board : true
    max_preset_tile : 1024
    tau : 0.95
    batch_size : 64
    buffer_size : 50000

  experiment_9:
    save_path : output/DQN_policy_exp9_guide_no_sheduler
    reward_func : Guide_score
    n_episodes : 20000
    hidden_dim : 2048
    target_update_interval : 20
    initial_epsilon : 0.9
    epsilon_decay : 0.985
    final_epsilon : 0.01
    learning_rate : 0.00005
    discount_factor :  0.99
    soft_update : false
    use_scheduler : false
    use_preset_board : false
    max_preset_tile : 4
    tau : 0.95
    batch_size : 64
    buffer_size : 50000

  play_1:
    save_path : play_output\DQN_full_score
    n_episodes : 300
    policy_network_weight : C:\STORAGE\FIBO\Sem_4_2\DRL\DRL_final\DQN_policy_exp1\policy_best.pth
    target_network_weight : C:\STORAGE\FIBO\Sem_4_2\DRL\DRL_final\DQN_policy_exp1\policy_best.pth
    hidden_dim : 2048
    target_update_interval : 20
    initial_epsilon : 0.9
    epsilon_decay : 0.985
    final_epsilon : 0.01
    learning_rate : 0.00005
    discount_factor :  0.99
    soft_update : false
    use_scheduler : false
    use_preset_board : false
    max_preset_tile : 4
    tau : 0.95
    batch_size : 64
    buffer_size : 50000

  play_2:
    save_path : play_output\DQN_merge_score
    n_episodes : 300
    policy_network_weight : C:\STORAGE\FIBO\Sem_4_2\DRL\DRL_final\DQN_policy_exp2\policy_best.pth
    target_network_weight : C:\STORAGE\FIBO\Sem_4_2\DRL\DRL_final\DQN_policy_exp2\policy_best.pth
    hidden_dim : 2048
    target_update_interval : 20
    initial_epsilon : 0.9
    epsilon_decay : 0.985
    final_epsilon : 0.01
    learning_rate : 0.00005
    discount_factor :  0.99
    soft_update : false
    use_scheduler : false
    use_preset_board : false
    max_preset_tile : 4
    tau : 0.95
    batch_size : 64
    buffer_size : 50000

  play_3:
    save_path : play_output\DQN_guide_score
    n_episodes : 300
    policy_network_weight : C:\STORAGE\FIBO\Sem_4_2\DRL\DRL_final\DQN_policy_exp4\policy_best.pth
    target_network_weight : C:\STORAGE\FIBO\Sem_4_2\DRL\DRL_final\DQN_policy_exp4\policy_best.pth
    hidden_dim : 2048
    target_update_interval : 20
    initial_epsilon : 0.9
    epsilon_decay : 0.985
    final_epsilon : 0.01
    learning_rate : 0.00005
    discount_factor :  0.99
    soft_update : false
    use_scheduler : false
    use_preset_board : false
    max_preset_tile : 4
    tau : 0.95
    batch_size : 64
    buffer_size : 50000

  play_4:
    save_path : play_output\DQN_preset_512
    n_episodes : 300
    policy_network_weight : C:\STORAGE\FIBO\Sem_4_2\DRL\DRL_final\DQN_policy_exp7_preset_512\policy_best.pth
    target_network_weight : C:\STORAGE\FIBO\Sem_4_2\DRL\DRL_final\DQN_policy_exp7_preset_512\policy_best.pth
    hidden_dim : 2048
    target_update_interval : 20
    initial_epsilon : 0.9
    epsilon_decay : 0.985
    final_epsilon : 0.01
    learning_rate : 0.00005
    discount_factor :  0.99
    soft_update : false
    use_scheduler : false
    use_preset_board : false
    max_preset_tile : 4
    tau : 0.95
    batch_size : 64
    buffer_size : 50000

  play_5:
    save_path : play_output\DQN_preset_1024
    n_episodes : 300
    policy_network_weight : C:\STORAGE\FIBO\Sem_4_2\DRL\DRL_final\DQN_policy_exp8_preset_1024\policy_best.pth
    target_network_weight : C:\STORAGE\FIBO\Sem_4_2\DRL\DRL_final\DQN_policy_exp8_preset_1024\policy_best.pth
    hidden_dim : 2048
    target_update_interval : 20
    initial_epsilon : 0.9
    epsilon_decay : 0.985
    final_epsilon : 0.01
    learning_rate : 0.00005
    discount_factor :  0.99
    soft_update : false
    use_scheduler : false
    use_preset_board : false
    max_preset_tile : 4
    tau : 0.95
    batch_size : 64
    buffer_size : 50000
  
  play_6:
    save_path : play_output\DQN_guide_score_no_scheduler
    n_episodes : 300
    policy_network_weight : C:\STORAGE\FIBO\Sem_4_2\DRL\DRL_final\DQN_policy_exp9_guide_no_sheduler\policy_best.pth
    target_network_weight : C:\STORAGE\FIBO\Sem_4_2\DRL\DRL_final\DQN_policy_exp9_guide_no_sheduler\policy_best.pth
    hidden_dim : 2048
    target_update_interval : 20
    initial_epsilon : 0.9
    epsilon_decay : 0.985
    final_epsilon : 0.01
    learning_rate : 0.00005
    discount_factor :  0.99
    soft_update : false
    use_scheduler : false
    use_preset_board : false
    max_preset_tile : 4
    tau : 0.95
    batch_size : 64
    buffer_size : 50000